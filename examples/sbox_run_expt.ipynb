{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run_expt.py contents\n",
    "\n",
    "## 1) Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4860.4765625\n"
     ]
    }
   ],
   "source": [
    "import os, psutil; print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8755d436b714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import pyBigWig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# %timeit bw = pyBigWig.open(\"/users/abalsubr/wilds/examples/data/encode-tfbs_v1.0/DNASE.K562.fc.signal.bigwig\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bw.values('chr1', 10000, 22800, numpy=True)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2334\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2335\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2336\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2337\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/wilds-final-3/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bw' is not defined"
     ]
    }
   ],
   "source": [
    "# import pyBigWig\n",
    "# %timeit bw = pyBigWig.open(\"/users/abalsubr/wilds/examples/data/encode-tfbs_v1.0/DNASE.K562.fc.signal.bigwig\")\n",
    "%timeit bw.values('chr1', 10000, 22800, numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.2.4, while the latest version is 1.2.6.\n"
     ]
    }
   ],
   "source": [
    "import os, csv, sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '4'\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np, pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import pyBigWig\n",
    "from collections import defaultdict\n",
    "\n",
    "from wilds.common.data_loaders import get_train_loader, get_eval_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "\n",
    "from utils import set_seed, Logger, BatchLogger, log_config, ParseKwargs, load, initialize_wandb, log_group_data, parse_bool\n",
    "from train import train, evaluate\n",
    "from algorithms.initializer import initialize_algorithm\n",
    "from transforms import initialize_transform\n",
    "from configs.utils import populate_defaults\n",
    "import configs.supported as supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--resume'], dest='resume', nargs='?', const=True, default=False, type=<function parse_bool at 0x7f0e6428c820>, choices=None, help=None, metavar=None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' set default hyperparams in default_hyperparams.py '''\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Required arguments\n",
    "parser.add_argument('-d', '--dataset', choices=supported.datasets, required=True)\n",
    "parser.add_argument('--algorithm', required=True, choices=supported.algorithms)\n",
    "parser.add_argument('--root_dir', required=True,\n",
    "                    help='The directory where [dataset]/data can be found (or should be downloaded to, if it does not exist).')\n",
    "\n",
    "# Dataset\n",
    "parser.add_argument('--split_scheme', help='Identifies how the train/val/test split is constructed. Choices are dataset-specific.')\n",
    "parser.add_argument('--dataset_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--download', default=False, type=parse_bool, const=True, nargs='?',\n",
    "                    help='If true, tries to downloads the dataset if it does not exist in root_dir.')\n",
    "parser.add_argument('--frac', type=float, default=1.0,\n",
    "                    help='Convenience parameter that scales all dataset splits down to the specified fraction, for development purposes.')\n",
    "\n",
    "# Loaders\n",
    "parser.add_argument('--loader_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--train_loader', choices=['standard', 'group'])\n",
    "parser.add_argument('--uniform_over_groups', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--distinct_groups', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--n_groups_per_batch', type=int)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--eval_loader', choices=['standard'], default='standard')\n",
    "\n",
    "# Model\n",
    "parser.add_argument('--model', choices=supported.models)\n",
    "parser.add_argument('--model_kwargs', nargs='*', action=ParseKwargs, default={},\n",
    "    help='keyword arguments for model initialization passed as key1=value1 key2=value2')\n",
    "\n",
    "# Transforms\n",
    "parser.add_argument('--train_transform', choices=supported.transforms)\n",
    "parser.add_argument('--eval_transform', choices=supported.transforms)\n",
    "parser.add_argument('--target_resolution', nargs='+', type=int, help='target resolution. for example --target_resolution 224 224 for standard resnet.')\n",
    "parser.add_argument('--resize_scale', type=float)\n",
    "parser.add_argument('--max_token_length', type=int)\n",
    "\n",
    "# Objective\n",
    "parser.add_argument('--loss_function', choices = supported.losses)\n",
    "\n",
    "# Algorithm\n",
    "parser.add_argument('--groupby_fields', nargs='+')\n",
    "parser.add_argument('--group_dro_step_size', type=float)\n",
    "parser.add_argument('--coral_penalty_weight', type=float)\n",
    "parser.add_argument('--irm_lambda', type=float)\n",
    "parser.add_argument('--irm_penalty_anneal_iters', type=int)\n",
    "parser.add_argument('--algo_log_metric')\n",
    "\n",
    "# Model selection\n",
    "parser.add_argument('--val_metric')\n",
    "parser.add_argument('--val_metric_decreasing', type=parse_bool, const=True, nargs='?')\n",
    "\n",
    "# Optimization\n",
    "parser.add_argument('--n_epochs', type=int)\n",
    "parser.add_argument('--optimizer', choices=supported.optimizers)\n",
    "parser.add_argument('--lr', type=float)\n",
    "parser.add_argument('--weight_decay', type=float)\n",
    "parser.add_argument('--max_grad_norm', type=float)\n",
    "parser.add_argument('--optimizer_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "\n",
    "# Scheduler\n",
    "parser.add_argument('--scheduler', choices=supported.schedulers)\n",
    "parser.add_argument('--scheduler_kwargs', nargs='*', action=ParseKwargs, default={})\n",
    "parser.add_argument('--scheduler_metric_split', choices=['train', 'val'], default='val')\n",
    "parser.add_argument('--scheduler_metric_name')\n",
    "\n",
    "# Evaluation\n",
    "parser.add_argument('--evaluate_all_splits', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--eval_splits', nargs='+', default=[])\n",
    "parser.add_argument('--eval_only', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--eval_epoch', default=None, type=int)\n",
    "\n",
    "# Misc\n",
    "parser.add_argument('--device', type=int, default=0)\n",
    "parser.add_argument('--seed', type=int, default=0)\n",
    "parser.add_argument('--log_dir', default='./logs')\n",
    "parser.add_argument('--log_every', default=50, type=int)\n",
    "parser.add_argument('--save_step', type=int)\n",
    "parser.add_argument('--save_best', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--save_last', type=parse_bool, const=True, nargs='?', default=True)\n",
    "parser.add_argument('--no_group_logging', type=parse_bool, const=True, nargs='?')\n",
    "parser.add_argument('--use_wandb', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--progress_bar', type=parse_bool, const=True, nargs='?', default=False)\n",
    "parser.add_argument('--resume', type=parse_bool, const=True, nargs='?', default=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "argstr_camelyon = \"--dataset camelyon17 --algorithm ERM --root_dir data\"\n",
    "# argstr_camelyon = \"--dataset civilcomments --algorithm ERM --root_dir data\"\n",
    "config_camelyon = parser.parse_args(argstr_camelyon.split())\n",
    "config_camelyon = populate_defaults(config_camelyon)\n",
    "\n",
    "argstr_encode = \"--dataset encode-tfbs --algorithm ERM --root_dir data\"\n",
    "config_encode = parser.parse_args(argstr_encode.split())\n",
    "config_encode = populate_defaults(config_encode)\n",
    "\n",
    "config = config_camelyon\n",
    "config = config_encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "argstr_camelyon = \"--dataset camelyon17 --algorithm ERM --root_dir data\"\n",
    "# argstr_camelyon = \"--dataset civilcomments --algorithm ERM --root_dir data\"\n",
    "config_camelyon = parser.parse_args(argstr_camelyon.split())\n",
    "\n",
    "argstr_encode = \"--dataset encode-tfbs --algorithm ERM --root_dir data\"\n",
    "config_encode = parser.parse_args(argstr_encode.split())\n",
    "config_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.optimizer_kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: encode-tfbs\n",
      "Algorithm: ERM\n",
      "Root dir: data\n",
      "Split scheme: official\n",
      "Dataset kwargs: {}\n",
      "Download: False\n",
      "Frac: 1.0\n",
      "Loader kwargs: {'num_workers': 1, 'pin_memory': True}\n",
      "Train loader: standard\n",
      "Uniform over groups: False\n",
      "Distinct groups: None\n",
      "N groups per batch: 2\n",
      "Batch size: 64\n",
      "Eval loader: standard\n",
      "Model: leopard\n",
      "Model kwargs: {'pretrained': False}\n",
      "Train transform: None\n",
      "Eval transform: None\n",
      "Target resolution: None\n",
      "Resize scale: None\n",
      "Max token length: None\n",
      "Loss function: multitask_bce\n",
      "Groupby fields: ['celltype']\n",
      "Group dro step size: None\n",
      "Coral penalty weight: None\n",
      "Irm lambda: None\n",
      "Irm penalty anneal iters: None\n",
      "Algo log metric: multitask_avgprec\n",
      "Val metric: acc_avg\n",
      "Val metric decreasing: False\n",
      "N epochs: 5\n",
      "Optimizer: Adam\n",
      "Lr: 0.001\n",
      "Weight decay: 0.01\n",
      "Max grad norm: None\n",
      "Optimizer kwargs: {}\n",
      "Scheduler: None\n",
      "Scheduler kwargs: {}\n",
      "Scheduler metric split: val\n",
      "Scheduler metric name: None\n",
      "Evaluate all splits: True\n",
      "Eval splits: []\n",
      "Eval only: False\n",
      "Eval epoch: None\n",
      "Device: cuda:0\n",
      "Seed: 0\n",
      "Log dir: ./logs\n",
      "Log every: 50\n",
      "Save step: None\n",
      "Save best: True\n",
      "Save last: True\n",
      "No group logging: False\n",
      "Use wandb: False\n",
      "Progress bar: False\n",
      "Resume: False\n",
      "\n",
      "chr3 2.9614717960357666\n",
      "chr2 6.587897777557373\n",
      "chr1 10.29332971572876\n"
     ]
    }
   ],
   "source": [
    "# set device\n",
    "config.device = torch.device(\"cuda:\" + str(config.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "## Initialize logs\n",
    "if os.path.exists(config.log_dir) and config.resume:\n",
    "    resume=True\n",
    "    mode='a'\n",
    "elif os.path.exists(config.log_dir) and config.eval_only:\n",
    "    resume=False\n",
    "    mode='a'\n",
    "else:\n",
    "    resume=False\n",
    "    mode='w'\n",
    "\n",
    "if not os.path.exists(config.log_dir):\n",
    "    os.makedirs(config.log_dir)\n",
    "logger = Logger(os.path.join(config.log_dir, 'log.txt'), mode)\n",
    "\n",
    "# Record config\n",
    "log_config(config, logger)\n",
    "\n",
    "# Set random seed\n",
    "set_seed(config.seed)\n",
    "\n",
    "# Data\n",
    "full_dataset = supported.datasets[config.dataset](\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)\n",
    "\n",
    "# To implement data augmentation (i.e., have different transforms\n",
    "# at training time vs. test time), modify these two lines:\n",
    "train_transform = initialize_transform(\n",
    "    transform_name=config.train_transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset)\n",
    "eval_transform = initialize_transform(\n",
    "    transform_name=config.eval_transform,\n",
    "    config=config,\n",
    "    dataset=full_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wilds.datasets.encodetfbs_dataset.EncodeTFBSDataset at 0x7f8c174737f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "full_dataset_camelyon17 = copy.deepcopy(full_dataset)\n",
    "\n",
    "# supported.datasets[config_encode.dataset]\n",
    "# print(config_camelyon.train_transform, config_encode.train_transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Initialize dataset object (trial version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr3 3.0055365562438965\n",
      "chr4 5.905960321426392\n",
      "chr5 8.651455879211426\n",
      "chr6 11.250766038894653\n",
      "chr7 13.660939931869507\n",
      "chr10 15.713522672653198\n",
      "chr12 17.740623474121094\n",
      "chr13 19.478207111358643\n",
      "chr14 21.088634252548218\n",
      "chr15 22.625713348388672\n",
      "chr16 23.987269639968872\n",
      "chr17 25.21428894996643\n",
      "chr18 26.394341230392456\n",
      "chr19 27.28497076034546\n",
      "chr20 28.235496282577515\n",
      "chr22 28.999913692474365\n",
      "chrX 31.338406085968018\n",
      "chr2 35.00527381896973\n",
      "chr9 37.12277841567993\n",
      "chr11 39.157737016677856\n",
      "chr1 42.89226841926575\n",
      "chr8 45.092690229415894\n",
      "chr21 45.81230306625366\n",
      "H1-hESC 45.81402635574341\n",
      "HCT116 45.814292192459106\n",
      "HeLa-S3 45.814526081085205\n",
      "HepG2 45.814810276031494\n",
      "K562 45.815062522888184\n",
      "A549 45.81636619567871\n",
      "GM12878 45.81674289703369\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wilds.datasets.wilds_dataset import WILDSDataset\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.common.metrics.all_metrics import Accuracy\n",
    "\n",
    "root_dir='data'\n",
    "download=False\n",
    "split_scheme='official'\n",
    "\n",
    "itime = time.time()\n",
    "_dataset_name = 'encode-tfbs'\n",
    "_version = '1.0'\n",
    "_download_url = 'https://worksheets.codalab.org/rest/bundles/0x8b3255e21e164cd98d3aeec09cd0bc26/contents/blob/'\n",
    "_data_dir = 'data/encode-tfbs_v1.0/'\n",
    "_y_size = 1\n",
    "_n_classes = 2\n",
    "\n",
    "_train_chroms = ['chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr10', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr22', 'chrX']\n",
    "_val_chroms = ['chr2', 'chr9', 'chr11']\n",
    "_test_chroms = ['chr1', 'chr8', 'chr21']\n",
    "_transcription_factor = 'MAX'\n",
    "_train_celltypes = ['H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562']\n",
    "_val_celltype = ['A549']\n",
    "_test_celltype = ['GM12878']\n",
    "_all_chroms = _train_chroms + _val_chroms + _test_chroms\n",
    "_all_celltypes = _train_celltypes + _val_celltype + _test_celltype\n",
    "\n",
    "_metadata_map = {}\n",
    "_metadata_map['chr'] = _all_chroms\n",
    "_metadata_map['celltype'] = _all_celltypes\n",
    "\n",
    "# Get the splits\n",
    "if split_scheme=='official':\n",
    "    split_scheme = 'standard'\n",
    "\n",
    "_split_scheme = split_scheme\n",
    "_split_dict = {\n",
    "    'train': 0,\n",
    "    'id_val': 1,\n",
    "    'test': 2,\n",
    "    'val': 3\n",
    "}\n",
    "_split_names = {\n",
    "    'train': 'Train',\n",
    "    'id_val': 'Validation (ID)',\n",
    "    'test': 'Test',\n",
    "    'val': 'Validation (OOD)',\n",
    "}\n",
    "\n",
    "# Load sequence and DNase features\n",
    "sequence_filename = os.path.join(_data_dir, 'sequence.npz')\n",
    "seq_arr = np.load(sequence_filename)\n",
    "_seq_bp = {}\n",
    "for chrom in _all_chroms:\n",
    "    _seq_bp[chrom] = seq_arr[chrom]\n",
    "    print(chrom, time.time() - itime)\n",
    "\n",
    "_dnase_allcelltypes = {}\n",
    "ct = 'avg'\n",
    "dnase_avg_bw_path = os.path.join(_data_dir, 'Leopard_dnase/{}.bigwig'.format(ct))\n",
    "_dnase_allcelltypes[ct] = pyBigWig.open(dnase_avg_bw_path)\n",
    "for ct in _all_celltypes:\n",
    "    \"\"\"\n",
    "    dnase_filename = os.path.join(self._data_dir, '{}_dnase.npz'.format(ct))\n",
    "    dnase_npz_contents = np.load(dnase_filename)\n",
    "    self._dnase_allcelltypes[ct] = {}\n",
    "    for chrom in self._all_chroms: #self._seq_bp:\n",
    "        self._dnase_allcelltypes[ct][chrom] = dnase_npz_contents[chrom]\n",
    "    \"\"\"\n",
    "    dnase_bw_path = os.path.join(_data_dir, 'Leopard_dnase/{}.bigwig'.format(ct))\n",
    "    _dnase_allcelltypes[ct] = pyBigWig.open(dnase_bw_path)\n",
    "    print(ct, time.time() - itime)\n",
    "\n",
    "_metadata_df = pd.read_csv(\n",
    "    _data_dir + 'labels/MAX/metadata_df.bed', sep='\\t', header=None, \n",
    "    index_col=None, names=['chr', 'start', 'stop', 'celltype']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_regions_mask = np.isin(_metadata_df['chr'], _train_chroms)\n",
    "val_regions_mask = np.isin(_metadata_df['chr'], _val_chroms)\n",
    "test_regions_mask = np.isin(_metadata_df['chr'], _test_chroms)\n",
    "train_celltype_mask = np.isin(_metadata_df['celltype'], _train_celltypes)\n",
    "val_celltype_mask = np.isin(_metadata_df['celltype'], _val_celltype)\n",
    "test_celltype_mask = np.isin(_metadata_df['celltype'], _test_celltype)\n",
    "\n",
    "split_array = -1*np.ones(_metadata_df.shape[0]).astype(int)\n",
    "split_array[np.logical_and(train_regions_mask, train_celltype_mask)] = _split_dict['train']\n",
    "split_array[np.logical_and(test_regions_mask, test_celltype_mask)] = _split_dict['test']\n",
    "# Validate using validation chr, either using a designated validation cell line ('val') or a training cell line ('id_val')\n",
    "split_array[np.logical_and(val_regions_mask, val_celltype_mask)] = _split_dict['val']\n",
    "split_array[np.logical_and(val_regions_mask, train_celltype_mask)] = _split_dict['id_val']\n",
    "\n",
    "if _split_scheme=='standard':\n",
    "    _metadata_df.insert(len(_metadata_df.columns), 'split', split_array)\n",
    "else:\n",
    "    raise ValueError(f'Split scheme {_split_scheme} not recognized')\n",
    "\n",
    "metadata_mask = (_metadata_df['split'] != -1)\n",
    "_metadata_df = _metadata_df[_metadata_df['split'] != -1]\n",
    "\n",
    "chr_ints = _metadata_df['chr'].replace(dict( [(y, x) for x, y in enumerate(_metadata_map['chr'])] )).values\n",
    "celltype_ints = _metadata_df['celltype'].replace(dict( [(y, x) for x, y in enumerate(_metadata_map['celltype'])] )).values\n",
    "_split_array = _metadata_df['split'].values\n",
    "\n",
    "_y_array = torch.Tensor(np.load(_data_dir + 'labels/MAX/metadata_y.npy'))\n",
    "_y_array = _y_array[metadata_mask]\n",
    "\n",
    "_metadata_array = torch.stack(\n",
    "    (torch.LongTensor(chr_ints), \n",
    "     torch.LongTensor(celltype_ints)\n",
    "    ),\n",
    "    dim=1)\n",
    "_metadata_fields = ['chr', 'celltype']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_label_vec(\n",
    "    metadata_df, seed_chr, seed_celltype, seed_start, output_size=128\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a coordinate in a celltype, gets the labels of \n",
    "    the `output_size` 200bp bins from that coordinate onward. \n",
    "    \"\"\"\n",
    "    itime = time.time()\n",
    "    \n",
    "    # Extract regions from this chromosome in this celltype, to get a window of labels from\n",
    "    # print(time.time() - itime)\n",
    "    # chr_msk = np.array(metadata_df['chr']) == seed_region['chr']\n",
    "    # print(time.time() - itime)\n",
    "    # ct_msk = np.array(metadata_df['celltype']) == seed_region['celltype']\n",
    "    # mdf = metadata_df[chr_msk & ct_msk]\n",
    "    seq_size = output_size*50\n",
    "    mdf = metadata_df.loc[\n",
    "        (metadata_df['chr'] == seed_chr) & \n",
    "        (metadata_df['celltype'] == seed_celltype) & \n",
    "        (metadata_df['start'] >= seed_start) & \n",
    "        (metadata_df['stop'] < seed_start+seq_size)\n",
    "    ]\n",
    "    print(time.time() - itime)\n",
    "\n",
    "    # Get labels\n",
    "    y_label_vec = np.zeros(output_size)\n",
    "    y_label_vec[(mdf['start'] - seed_start) // 50] = mdf['y']\n",
    "    return mdf, y_label_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset object (long version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wilds.datasets.wilds_dataset import WILDSDataset\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "from wilds.common.metrics.all_metrics import Accuracy\n",
    "\n",
    "class EncodeTFBSDataset(WILDSDataset):\n",
    "    \"\"\"\n",
    "    ENCODE-DREAM-wilds dataset of transcription factor binding sites. \n",
    "    This is a subset of the dataset from the ENCODE-DREAM in vivo Transcription Factor Binding Site Prediction Challenge. \n",
    "    \n",
    "    Input (x):\n",
    "        1000-base-pair regions of sequence with a quantified chromatin accessibility readout.\n",
    "\n",
    "    Label (y):\n",
    "        y is binary. It is 1 if the central 200bp region is bound by the transcription factor MAX, and 0 otherwise.\n",
    "\n",
    "    Metadata:\n",
    "        Each sequence is annotated with the celltype of origin (a string) and the chromosome of origin (a string).\n",
    "    \n",
    "    Website:\n",
    "        https://www.synapse.org/#!Synapse:syn6131484\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root_dir='data', download=False, split_scheme='official'):\n",
    "        itime = time.time()\n",
    "        self._dataset_name = 'encode-tfbs'\n",
    "        self._version = '1.0'\n",
    "        self._download_url = 'https://worksheets.codalab.org/rest/bundles/0x8b3255e21e164cd98d3aeec09cd0bc26/contents/blob/'\n",
    "        self._data_dir = self.initialize_data_dir(root_dir, download)\n",
    "        self._y_size = 128\n",
    "        # self._n_classes = 2\n",
    "        \n",
    "        self._train_chroms = ['chr3', 'chr4', 'chr5', 'chr6', 'chr7', 'chr10', 'chr12', 'chr13', 'chr14', 'chr15', 'chr16', 'chr17', 'chr18', 'chr19', 'chr20', 'chr22', 'chrX']\n",
    "        self._val_chroms = ['chr2', 'chr9', 'chr11']\n",
    "        self._test_chroms = ['chr1', 'chr8', 'chr21']\n",
    "        self._transcription_factor = 'MAX'\n",
    "        self._train_celltypes = ['H1-hESC', 'HCT116', 'HeLa-S3', 'HepG2', 'K562']\n",
    "        self._val_celltype = ['A549']\n",
    "        self._test_celltype = ['GM12878']\n",
    "        self._all_chroms = self._train_chroms + self._val_chroms + self._test_chroms\n",
    "        self._all_celltypes = self._train_celltypes + self._val_celltype + self._test_celltype\n",
    "        \n",
    "        self._metadata_map = {}\n",
    "        self._metadata_map['chr'] = self._all_chroms\n",
    "        self._metadata_map['celltype'] = self._all_celltypes\n",
    "        \n",
    "        # Get the splits\n",
    "        if split_scheme=='official':\n",
    "            split_scheme = 'standard'\n",
    "        \n",
    "        self._split_scheme = split_scheme\n",
    "        self._split_dict = {\n",
    "            'train': 0,\n",
    "            'id_val': 1,\n",
    "            'test': 2,\n",
    "            'val': 3\n",
    "        }\n",
    "        self._split_names = {\n",
    "            'train': 'Train',\n",
    "            'id_val': 'Validation (ID)',\n",
    "            'test': 'Test',\n",
    "            'val': 'Validation (OOD)',\n",
    "        }\n",
    "        \n",
    "        # Load sequence and DNase features\n",
    "        sequence_filename = os.path.join(self._data_dir, 'sequence.npz')\n",
    "        seq_arr = np.load(sequence_filename)\n",
    "        self._seq_bp = {}\n",
    "        for chrom in self._all_chroms: #seq_arr:\n",
    "            self._seq_bp[chrom] = seq_arr[chrom]\n",
    "            print(chrom, time.time() - itime)\n",
    "        \n",
    "        self._dnase_allcelltypes = {}\n",
    "        ct = 'avg'\n",
    "        dnase_avg_bw_path = os.path.join(self._data_dir, 'Leopard_dnase/{}.bigwig'.format(ct))\n",
    "        self._dnase_allcelltypes[ct] = pyBigWig.open(dnase_avg_bw_path)\n",
    "        for ct in self._all_celltypes:\n",
    "            \"\"\"\n",
    "            dnase_filename = os.path.join(self._data_dir, '{}_dnase.npz'.format(ct))\n",
    "            dnase_npz_contents = np.load(dnase_filename)\n",
    "            self._dnase_allcelltypes[ct] = {}\n",
    "            for chrom in self._all_chroms: #self._seq_bp:\n",
    "                self._dnase_allcelltypes[ct][chrom] = dnase_npz_contents[chrom]\n",
    "            \"\"\"\n",
    "            dnase_bw_path = os.path.join(self._data_dir, 'Leopard_dnase/{}.bigwig'.format(ct))\n",
    "            self._dnase_allcelltypes[ct] = pyBigWig.open(dnase_bw_path)\n",
    "        \n",
    "        self._metadata_df = pd.read_csv(\n",
    "            self._data_dir + '/labels/MAX/metadata_df.bed', sep='\\t', header=None, \n",
    "            index_col=None, names=['chr', 'start', 'stop', 'celltype']\n",
    "        )\n",
    "        \n",
    "        train_regions_mask = np.isin(self._metadata_df['chr'], self._train_chroms)\n",
    "        val_regions_mask = np.isin(self._metadata_df['chr'], self._val_chroms)\n",
    "        test_regions_mask = np.isin(self._metadata_df['chr'], self._test_chroms)\n",
    "        train_celltype_mask = np.isin(self._metadata_df['celltype'], self._train_celltypes)\n",
    "        val_celltype_mask = np.isin(self._metadata_df['celltype'], self._val_celltype)\n",
    "        test_celltype_mask = np.isin(self._metadata_df['celltype'], self._test_celltype)\n",
    "        \n",
    "        split_array = -1*np.ones(self._metadata_df.shape[0]).astype(int)\n",
    "        split_array[np.logical_and(train_regions_mask, train_celltype_mask)] = self._split_dict['train']\n",
    "        split_array[np.logical_and(test_regions_mask, test_celltype_mask)] = self._split_dict['test']\n",
    "        # Validate using validation chr, either using a designated validation cell line ('val') or a training cell line ('id_val')\n",
    "        split_array[np.logical_and(val_regions_mask, val_celltype_mask)] = self._split_dict['val']\n",
    "        split_array[np.logical_and(val_regions_mask, train_celltype_mask)] = self._split_dict['id_val']\n",
    "        \n",
    "        if self._split_scheme=='standard':\n",
    "            self._metadata_df.insert(len(self._metadata_df.columns), 'split', split_array)\n",
    "        else:\n",
    "            raise ValueError(f'Split scheme {self._split_scheme} not recognized')\n",
    "        \n",
    "        metadata_mask = (self._metadata_df['split'] != -1)\n",
    "        self._metadata_df = self._metadata_df[self._metadata_df['split'] != -1]\n",
    "        \n",
    "        chr_ints = self._metadata_df['chr'].replace(dict( [(y, x) for x, y in enumerate(self._metadata_map['chr'])] )).values\n",
    "        celltype_ints = self._metadata_df['celltype'].replace(dict( [(y, x) for x, y in enumerate(self._metadata_map['celltype'])] )).values\n",
    "        self._split_array = self._metadata_df['split'].values\n",
    "        self._y_array = torch.Tensor(np.load(self._data_dir + '/labels/MAX/metadata_y.npy'))\n",
    "        self._y_array = self._y_array[metadata_mask]\n",
    "        \n",
    "        self._metadata_array = torch.stack(\n",
    "            (torch.LongTensor(chr_ints), \n",
    "             torch.LongTensor(celltype_ints)\n",
    "            ),\n",
    "            dim=1)\n",
    "        self._metadata_fields = ['chr', 'celltype']\n",
    "        \n",
    "        self._eval_grouper = CombinatorialGrouper(\n",
    "            dataset=self,\n",
    "            groupby_fields=['celltype'])\n",
    "        \n",
    "        self._metric = Accuracy()\n",
    "        \n",
    "        super().__init__(root_dir, download, split_scheme)\n",
    "    \n",
    "    \"\"\"\n",
    "    def get_random_label_vec(metadata_df, output_size=128):\n",
    "        # Sample a positively labeled region at random\n",
    "        pos_mdf = metadata_df[metadata_df['y'] == 1] #.iloc[ metadata_df['chr'] == s['chr'], : ]\n",
    "        pos_seed_region = pos_mdf.iloc[np.random.randint(pos_mdf.shape[0])]\n",
    "\n",
    "        # Extract regions from this chromosome in this celltype, to get a window of labels from\n",
    "        chr_msk = np.array(metadata_df['chr']) == pos_seed_region['chr']\n",
    "        ct_msk = np.array(metadata_df['celltype']) == pos_seed_region['celltype']\n",
    "        mdf = metadata_df[chr_msk & ct_msk]\n",
    "\n",
    "        # Get labels\n",
    "        start_ndx = np.where(mdf['start'] == pos_seed_region['start'])[0][0]\n",
    "        y_label_vec = mdf.iloc[start_ndx:start_ndx+output_size, :]['y']\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_input(self, idx, window_size=12800):\n",
    "        \"\"\"\n",
    "        Returns x for a given idx in metadata_array, which has been filtered to only take windows with the desired stride.\n",
    "        Computes this from: \n",
    "        (1) sequence features in self._seq_bp\n",
    "        (2) DNase bigwig file handles in self._dnase_allcelltypes\n",
    "        (3) Metadata for the index (location along the genome with 6400bp window width)\n",
    "        (4) Window_size, the length of sequence returned (centered on the 6400bp region in (3))\n",
    "        \"\"\"\n",
    "        this_metadata = self._metadata_df.iloc[idx, :]\n",
    "        interval_start = this_metadata['start'] - int(window_size/4)\n",
    "        interval_end = interval_start + window_size  #this_metadata['stop']\n",
    "        seq_this = self._seq_bp[this_metadata['chr']][interval_start:interval_end]\n",
    "        dnase_bw = self._dnase_allcelltypes[this_metadata['celltype']]\n",
    "        dnase_this = dnase_bw.values(chrom, interval_start, interval_end, numpy=True)\n",
    "        dnase_avg = self._dnase_allcelltypes['avg'].values(chrom, interval_start, interval_end, numpy=True)\n",
    "        return torch.tensor(np.column_stack(\n",
    "            [np.nan_to_num(seq_this), np.nan_to_num(dnase_this), np.nan_to_num(dnase_avg)]\n",
    "        ))\n",
    "\n",
    "    def eval(self, y_pred, y_true, metadata):\n",
    "        return self.standard_group_eval(\n",
    "            self._metric,\n",
    "            self._eval_grouper,\n",
    "            y_pred, y_true, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr3 3.0425407886505127\n",
      "chr4 5.967821359634399\n",
      "chr5 8.747126340866089\n",
      "chr6 11.370141744613647\n",
      "chr7 13.802208423614502\n",
      "chr10 15.875979900360107\n",
      "chr12 17.929850339889526\n",
      "chr13 19.67976665496826\n",
      "chr14 21.306750059127808\n",
      "chr15 22.866544723510742\n",
      "chr16 24.241100788116455\n",
      "chr17 25.480982303619385\n",
      "chr18 26.677065134048462\n",
      "chr19 27.579110622406006\n",
      "chr20 28.545915603637695\n",
      "chr22 29.323810577392578\n",
      "chrX 31.698036670684814\n",
      "chr2 35.40705943107605\n",
      "chr9 37.5518524646759\n",
      "chr11 39.61783218383789\n",
      "chr1 43.411964893341064\n",
      "chr8 45.64823389053345\n",
      "chr21 46.377281188964844\n"
     ]
    }
   ],
   "source": [
    "full_dataset_encode = EncodeTFBSDataset(\n",
    "    root_dir=config.root_dir,\n",
    "    download=config.download,\n",
    "    split_scheme=config.split_scheme,\n",
    "    **config.dataset_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data...\n",
      "    celltype = H1-hESC: n = 5314\n",
      "    celltype = HCT116: n = 4759\n",
      "    celltype = HeLa-S3: n = 4635\n",
      "    celltype = HepG2: n = 4459\n",
      "    celltype = K562: n = 5169\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 0\n",
      "Validation (ID) data...\n",
      "    celltype = H1-hESC: n = 6872\n",
      "    celltype = HCT116: n = 6315\n",
      "    celltype = HeLa-S3: n = 4219\n",
      "    celltype = HepG2: n = 8356\n",
      "    celltype = K562: n = 6538\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 0\n",
      "Test data...\n",
      "    celltype = H1-hESC: n = 0\n",
      "    celltype = HCT116: n = 0\n",
      "    celltype = HeLa-S3: n = 0\n",
      "    celltype = HepG2: n = 0\n",
      "    celltype = K562: n = 0\n",
      "    celltype = A549: n = 0\n",
      "    celltype = GM12878: n = 4487\n",
      "Validation (OOD) data...\n",
      "    celltype = H1-hESC: n = 0\n",
      "    celltype = HCT116: n = 0\n",
      "    celltype = HeLa-S3: n = 0\n",
      "    celltype = HepG2: n = 0\n",
      "    celltype = K562: n = 0\n",
      "    celltype = A549: n = 6728\n",
      "    celltype = GM12878: n = 0\n",
      "Dout: 128\n"
     ]
    }
   ],
   "source": [
    "# config = config_encode\n",
    "\n",
    "train_grouper = CombinatorialGrouper(\n",
    "    dataset=full_dataset,\n",
    "    groupby_fields=config.groupby_fields)\n",
    "\n",
    "datasets = defaultdict(dict)\n",
    "for split in full_dataset.split_dict.keys():\n",
    "    if split=='train':\n",
    "        transform = train_transform\n",
    "        verbose = True\n",
    "    elif split == 'val':\n",
    "        transform = eval_transform\n",
    "        verbose = True\n",
    "    else:\n",
    "        transform = eval_transform\n",
    "        verbose = False\n",
    "    # Get subset\n",
    "    datasets[split]['dataset'] = full_dataset.get_subset(\n",
    "        split,\n",
    "        frac=config.frac,\n",
    "        transform=transform)\n",
    "\n",
    "    if split == 'train':\n",
    "        datasets[split]['loader'] = get_train_loader(\n",
    "            loader=config.train_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            batch_size=config.batch_size,\n",
    "            uniform_over_groups=config.uniform_over_groups,\n",
    "            grouper=train_grouper,\n",
    "            distinct_groups=config.distinct_groups,\n",
    "            n_groups_per_batch=config.n_groups_per_batch,\n",
    "            **config.loader_kwargs)\n",
    "    else:\n",
    "        datasets[split]['loader'] = get_eval_loader(\n",
    "            loader=config.eval_loader,\n",
    "            dataset=datasets[split]['dataset'],\n",
    "            grouper=train_grouper,\n",
    "            batch_size=config.batch_size,\n",
    "            **config.loader_kwargs)\n",
    "\n",
    "    # Set fields\n",
    "    datasets[split]['split'] = split\n",
    "    datasets[split]['name'] = full_dataset.split_names[split]\n",
    "    datasets[split]['verbose'] = verbose\n",
    "    # Loggers\n",
    "    # Loggers\n",
    "    datasets[split]['eval_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_eval.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "    datasets[split]['algo_logger'] = BatchLogger(\n",
    "        os.path.join(config.log_dir, f'{split}_algo.csv'), mode=mode, use_wandb=(config.use_wandb and verbose))\n",
    "\n",
    "    if config.use_wandb:\n",
    "        initialize_wandb(config)\n",
    "\n",
    "# Logging dataset info\n",
    "if config.no_group_logging and full_dataset.is_classification and full_dataset.y_size==1:\n",
    "    log_grouper = CombinatorialGrouper(\n",
    "        dataset=full_dataset,\n",
    "        groupby_fields=['y'])\n",
    "elif config.no_group_logging:\n",
    "    log_grouper = None\n",
    "else:\n",
    "    log_grouper = train_grouper\n",
    "log_group_data(datasets, log_grouper, logger)\n",
    "\n",
    "## Initialize algorithm\n",
    "algorithm = initialize_algorithm(\n",
    "    config=config,\n",
    "    datasets=datasets,\n",
    "    train_grouper=train_grouper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in datasets['train']['loader']:\n",
    "    x, y_true, metadata = batch\n",
    "    break\n",
    "# x = torch.transpose(x, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = algorithm.process_batch(batch)\n",
    "# algorithm.loss.compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7212, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = algorithm.loss.compute(d['y_pred'], d['y_true'], return_dict=False)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chr</th>\n",
       "      <th>start</th>\n",
       "      <th>stop</th>\n",
       "      <th>celltype</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39413</th>\n",
       "      <td>chr2</td>\n",
       "      <td>10003200</td>\n",
       "      <td>10009600</td>\n",
       "      <td>A549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39414</th>\n",
       "      <td>chr2</td>\n",
       "      <td>100032000</td>\n",
       "      <td>100038400</td>\n",
       "      <td>A549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39415</th>\n",
       "      <td>chr2</td>\n",
       "      <td>100102400</td>\n",
       "      <td>100108800</td>\n",
       "      <td>A549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39416</th>\n",
       "      <td>chr2</td>\n",
       "      <td>100172800</td>\n",
       "      <td>100179200</td>\n",
       "      <td>A549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39417</th>\n",
       "      <td>chr2</td>\n",
       "      <td>100230400</td>\n",
       "      <td>100236800</td>\n",
       "      <td>A549</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495287</th>\n",
       "      <td>chr3</td>\n",
       "      <td>9996800</td>\n",
       "      <td>10003200</td>\n",
       "      <td>K562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495288</th>\n",
       "      <td>chr3</td>\n",
       "      <td>99974400</td>\n",
       "      <td>99980800</td>\n",
       "      <td>K562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495289</th>\n",
       "      <td>chr3</td>\n",
       "      <td>99980800</td>\n",
       "      <td>99987200</td>\n",
       "      <td>K562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495290</th>\n",
       "      <td>chr3</td>\n",
       "      <td>99987200</td>\n",
       "      <td>99993600</td>\n",
       "      <td>K562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495291</th>\n",
       "      <td>chr3</td>\n",
       "      <td>99993600</td>\n",
       "      <td>100000000</td>\n",
       "      <td>K562</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67851 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         chr      start       stop celltype  split\n",
       "39413   chr2   10003200   10009600     A549      3\n",
       "39414   chr2  100032000  100038400     A549      3\n",
       "39415   chr2  100102400  100108800     A549      3\n",
       "39416   chr2  100172800  100179200     A549      3\n",
       "39417   chr2  100230400  100236800     A549      3\n",
       "...      ...        ...        ...      ...    ...\n",
       "495287  chr3    9996800   10003200     K562      0\n",
       "495288  chr3   99974400   99980800     K562      0\n",
       "495289  chr3   99980800   99987200     K562      0\n",
       "495290  chr3   99987200   99993600     K562      0\n",
       "495291  chr3   99993600  100000000     K562      0\n",
       "\n",
       "[67851 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.unique(full_dataset._metadata_df['split'], return_counts=True)\n",
    "full_dataset._metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0. , 0.5, 1. ], dtype=float32), array([7422683, 1007200,  255045]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(full_dataset.y_array, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8546625832706961"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7422683/8684928"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [0]:\n",
      "\n",
      "Train:\n",
      "torch.Size([8192]) torch.Size([8192]) torch.Size([64, 128]) torch.Size([64, 128])\n",
      "torch.Size([]) torch.Size([8192]) torch.Size([64, 128]) torch.Size([64, 128])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ffde10eb87f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     train(\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0malgorithm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(algorithm, datasets, general_logger, config, epoch_offset, best_val_metric)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;31m# First run training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneral_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# Then run val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/train.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(algorithm, dataset, general_logger, epoch, config, train)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mbatch_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/algorithms/single_model_algorithm.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;31m# log results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_log\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msanitize_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/examples/algorithms/group_algorithm.py\u001b[0m in \u001b[0;36mupdate_log\u001b[0;34m(self, results)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogged_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_group_logging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                     group_metrics, group_counts, worst_group_metric = m.compute_group_wise(\n\u001b[0m\u001b[1;32m     50\u001b[0m                         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_true'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/wilds/common/metrics/metric.py\u001b[0m in \u001b[0;36mcompute_group_wise\u001b[0;34m(self, y_pred, y_true, g, n_groups, return_dict)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;34m-\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDictionary\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0mgroup_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworst_group_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_group_wise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/wilds/common/metrics/metric.py\u001b[0m in \u001b[0;36m_compute_group_wise\u001b[0;34m(self, y_pred, y_true, g, n_groups)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0mflattened_g\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_metrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_g\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mgroup_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavg_over_groups\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflattened_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0mworst_group_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgroup_counts\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgroup_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_counts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworst_group_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/wilds/wilds/common/utils.py\u001b[0m in \u001b[0;36mavg_over_groups\u001b[0;34m(v, g, n_groups)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0mgroup_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0mgroup_avgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch_scatter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_groups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not config.eval_only:\n",
    "    ## Load saved results if resuming\n",
    "    resume_success = False\n",
    "    if resume:\n",
    "        save_path = os.path.join(config.log_dir, 'last_model.pth')\n",
    "        if not os.path.exists(save_path):\n",
    "            epochs = [\n",
    "                int(file.split('_')[0])\n",
    "                for file in os.listdir(config.log_dir) if file.endswith('.pth')]\n",
    "            if len(epochs) > 0:\n",
    "                latest_epoch = max(epochs)\n",
    "                save_path = os.path.join(config.log_dir, f'{latest_epoch}_model.pth')\n",
    "        try:\n",
    "            prev_epoch, best_val_metric = load(algorithm, save_path)\n",
    "            epoch_offset = prev_epoch + 1\n",
    "            logger.write(f'Resuming from epoch {epoch_offset} with best val metric {best_val_metric}')\n",
    "            resume_success = True\n",
    "        except FileNotFoundError:\n",
    "            pass\n",
    "\n",
    "    if resume_success == False:\n",
    "        epoch_offset=0\n",
    "        best_val_metric=None\n",
    "\n",
    "\n",
    "    train(\n",
    "        algorithm=algorithm,\n",
    "        datasets=datasets,\n",
    "        general_logger=logger,\n",
    "        config=config,\n",
    "        epoch_offset=epoch_offset,\n",
    "        best_val_metric=best_val_metric)\n",
    "else:\n",
    "    if config.eval_epoch is None:\n",
    "        eval_model_path = os.path.join(config.log_dir, 'best_model.pth')\n",
    "    else:\n",
    "        eval_model_path = os.path.join(config.log_dir, f'{config.eval_epoch}_model.pth')\n",
    "    best_epoch, best_val_metric = load(algorithm, eval_model_path)\n",
    "    if config.eval_epoch is None:\n",
    "        epoch = best_epoch\n",
    "    else:\n",
    "        epoch = config.eval_epoch\n",
    "    evaluate(\n",
    "        algorithm=algorithm,\n",
    "        datasets=datasets,\n",
    "        epoch=epoch,\n",
    "        general_logger=logger,\n",
    "        config=config)\n",
    "\n",
    "logger.close()\n",
    "for split in datasets:\n",
    "    datasets[split]['eval_logger'].close()\n",
    "    datasets[split]['algo_logger'].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
